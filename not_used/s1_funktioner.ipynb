{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-916eab3a0aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from . import get_functions\n",
    "from . import tools\n",
    "\n",
    "# TODO: Decide the amount of checking and control in the class\n",
    "\n",
    "\n",
    "class SarImage:\n",
    "    \"\"\" Class to contain SAR image, relevant meta data and methods.\n",
    "    Attributes:\n",
    "        bands(list of numpy arrays): The measurements.\n",
    "        mission(str): Mission name:\n",
    "        time(datetime): start time of acquisition\n",
    "        footprint(dict): dictionary with footprint of image\n",
    "                        footprint = {'latitude': np.array\n",
    "                                    'longitude': np.array}\n",
    "        product_meta(dict): Dictionary with meta data.\n",
    "        band_names(list of str): Names of the band. Normally the polarisation.\n",
    "        calibration_tables(list of dict): Dictionary with calibration_tables information for each band.\n",
    "        geo_tie_point(list of dict): Dictionary with geo tie point for each band.\n",
    "        band_meta(list of dict): Dictionary with meta data for each band.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bands, mission=None, time=None, footprint=None, product_meta=None,\n",
    "                 band_names=None, calibration_tables=None, geo_tie_point=None, band_meta=None, unit=None):\n",
    "\n",
    "        # assign values\n",
    "        self.bands = bands\n",
    "        self.mission = mission\n",
    "        self.time = time\n",
    "        self.footprint = footprint\n",
    "        self.product_meta = product_meta\n",
    "        self.band_names = band_names\n",
    "        self.calibration_tables = calibration_tables\n",
    "        self.geo_tie_point = geo_tie_point\n",
    "        self.band_meta = band_meta\n",
    "        self.unit = unit\n",
    "        # Note that SlC is in strips. Maybe load as list of images\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Mission: %s \\n Bands: %s\" % (self.mission, str(self.band_names))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Overload the get and slicing function a[2,4] a[:10,3:34]\n",
    "\n",
    "        # Check i 2 dimension are given\n",
    "        if len(key) != 2:\n",
    "            raise ValueError('Need to slice both column and row like test_image[:,:]')\n",
    "\n",
    "        # Get values as array\n",
    "        if isinstance(key[0], int) & isinstance(key[1], int):\n",
    "            return [band[key] for band in self.bands]\n",
    "\n",
    "        if not isinstance(key[0], slice) & isinstance(key[1], slice):\n",
    "            raise ValueError('Only get at slice is supported: a[2,4] a[:10,3:34]')\n",
    "\n",
    "        # Else. Try to slice the image\n",
    "        slice_row = key[0]\n",
    "        slice_column = key[1]\n",
    "\n",
    "        row_start = slice_row.start\n",
    "        row_step = slice_row.step\n",
    "        row_stop = slice_row.stop\n",
    "\n",
    "        column_start = slice_column.start\n",
    "        column_step = slice_column.step\n",
    "        column_stop = slice_column.stop\n",
    "\n",
    "        if row_start is None:\n",
    "            row_start = 0\n",
    "        if row_step is None:\n",
    "            row_step = 1\n",
    "        if row_stop is None:\n",
    "            row_stop = self.bands[0].shape[0]\n",
    "        if column_start is None:\n",
    "            column_start = 0\n",
    "        if column_step is None:\n",
    "            column_step = 1\n",
    "        if column_stop is None:\n",
    "            column_stop = self.bands[0].shape[1]\n",
    "\n",
    "        # Adjust footprint to window\n",
    "        footprint_lat = np.zeros(4)\n",
    "        footprint_long = np.zeros(4)\n",
    "\n",
    "        window = ((row_start, row_stop), (column_start, column_stop))\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                lat_i, long_i = self.get_coordinate(window[0][i], window[1][j])\n",
    "                footprint_lat[2 * i + j] = lat_i\n",
    "                footprint_long[2 * i + j] = long_i\n",
    "\n",
    "        footprint = {'latitude': footprint_lat, 'longitude': footprint_long}\n",
    "\n",
    "        # Adjust geo_tie_point, calibration_tables\n",
    "        n_bands = len(self.bands)\n",
    "        geo_tie_point = copy.deepcopy(self.geo_tie_point)\n",
    "        calibration_tables = copy.deepcopy(self.calibration_tables)\n",
    "        for i in range(n_bands):\n",
    "            geo_tie_point[i]['row'] = (geo_tie_point[i]['row'] - row_start)/row_step\n",
    "            geo_tie_point[i]['column'] = (geo_tie_point[i]['column'] - column_start)/column_step\n",
    "\n",
    "            calibration_tables[i]['row'] = (calibration_tables[i]['row'] - row_start)/row_step\n",
    "            calibration_tables[i]['column'] = (calibration_tables[i]['column'] - column_start)/column_step\n",
    "\n",
    "        # slice the bands\n",
    "        bands = [band[key] for band in self.bands]\n",
    "\n",
    "        return SarImage(bands, mission=self.mission, time=self.time,\n",
    "                        footprint=footprint, product_meta=self.product_meta,\n",
    "                        band_names=self.band_names, calibration_tables=calibration_tables,\n",
    "                        geo_tie_point=geo_tie_point, band_meta=self.band_meta)\n",
    "\n",
    "    def get_index(self, lat, long):\n",
    "        \"\"\"Get index of a location by interpolating grid-points\n",
    "        Args:\n",
    "            lat(number): Latitude of the location\n",
    "            long(number): Longitude of location\n",
    "        Returns:\n",
    "            row(int): The row index of the location\n",
    "            column(int): The column index of the location\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        geo_tie_point = self.geo_tie_point\n",
    "        row = np.zeros(len(geo_tie_point), dtype=int)\n",
    "        column = np.zeros(len(geo_tie_point), dtype=int)\n",
    "\n",
    "        # find index for each band\n",
    "        for i in range(len(geo_tie_point)):\n",
    "            lat_grid = geo_tie_point[i]['latitude']\n",
    "            long_grid = geo_tie_point[i]['longitude']\n",
    "            row_grid = geo_tie_point[i]['row']\n",
    "            column_grid = geo_tie_point[i]['column']\n",
    "            row[i], column[i] = get_functions.get_index_v2(lat, long, lat_grid, long_grid, row_grid, column_grid)\n",
    "\n",
    "        # check that the results are the same\n",
    "        if (abs(row.max() - row.min()) > 0.5) or (abs(column.max() - column.min()) > 0.5):\n",
    "            warnings.warn('Warning different index found for each band. First index returned')\n",
    "\n",
    "        return row[0], column[0]\n",
    "\n",
    "    def get_coordinate(self, row, column):\n",
    "        \"\"\"Get coordinate from index by interpolating grid-points\n",
    "            Args:\n",
    "                row(number): index of the row of interest position\n",
    "                column(number): index of the column of interest position\n",
    "            Returns:\n",
    "                lat(float): Latitude of the position\n",
    "                long(float): longitude of the position\n",
    "            Raises:\n",
    "            \"\"\"\n",
    "\n",
    "        geo_tie_point = self.geo_tie_point\n",
    "        lat = np.zeros(len(geo_tie_point), dtype=float)\n",
    "        long = np.zeros(len(geo_tie_point), dtype=float)\n",
    "\n",
    "        # find index for each band\n",
    "        for i in range(len(geo_tie_point)):\n",
    "            lat_grid = geo_tie_point[i]['latitude']\n",
    "            long_grid = geo_tie_point[i]['longitude']\n",
    "            row_grid = geo_tie_point[i]['row']\n",
    "            column_grid = geo_tie_point[i]['column']\n",
    "            lat[i], long[i] = get_functions.get_coordinate(row, column, lat_grid, long_grid, row_grid, column_grid)\n",
    "\n",
    "        # check that the results are the same\n",
    "        if (abs(lat.max() - lat.min()) > 0.001) or (abs(long.max() - long.min()) > 0.001):\n",
    "            warnings.warn('Warning different coordinates found for each band. Mean returned')\n",
    "\n",
    "        return lat.mean(), long.mean()\n",
    "\n",
    "    def simple_plot(self, band_index=0, q_max=0.95, stride=1, **kwargs):\n",
    "        \"\"\" Makes a simple image of band and a color bar.\n",
    "            Args:\n",
    "                band_index(int): index of the band to plot.\n",
    "                q_max(number): q_max is the quantile used to set the max of the color range for example\n",
    "                                q_max = 0.95 shows the lowest 95 percent of pixel values in the color range\n",
    "                stride(int): Used to skip pixels when showing. Good for large images.\n",
    "                **kwargs: Passed on to matplotlib imshow\n",
    "            Returns:\n",
    "            Raises:\n",
    "            \"\"\"\n",
    "        v_max = np.quantile(self.bands[band_index].reshape(-1), q_max)\n",
    "\n",
    "        plt.imshow(self.bands[band_index][::stride, ::stride], vmax=v_max, **kwargs)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        return\n",
    "\n",
    "    def calibrate(self, mode='gamma', tiles=4):\n",
    "        \"\"\"Get coordinate from index by interpolating grid-points\n",
    "        Args:\n",
    "            mode(string): 'sigma_0', 'beta' or 'gamma'\n",
    "            tiles(int): number of tiles the image is divided into. This saves memory but reduce speed a bit\n",
    "        Returns:\n",
    "            Calibrated image as (SarImage)\n",
    "        Raises:\n",
    "        \"\"\"\n",
    "        if 'raw' not in self.unit:\n",
    "            warnings.warn('Raw is not in units. The image have all ready been calibrated')\n",
    "        \n",
    "        calibrated_bands = []\n",
    "        for i, band in enumerate(self.bands):\n",
    "            row = self.calibration_tables[i]['row']\n",
    "            column = self.calibration_tables[i]['column']\n",
    "            calibration_values = self.calibration_tables[i][mode]\n",
    "            calibrated_bands.append(tools.calibration(band, row, column, calibration_values, tiles=tiles))\n",
    "\n",
    "        return SarImage(calibrated_bands, mission=self.mission, time=self.time,\n",
    "                        footprint=self.footprint, product_meta=self.product_meta,\n",
    "                        band_names=self.band_names, calibration_tables=self.calibration_tables,\n",
    "                        geo_tie_point=self.geo_tie_point, band_meta=self.band_meta,\n",
    "                        unit=mode)\n",
    "\n",
    "    def to_db(self):\n",
    "        \"\"\"Convert  to decibel\n",
    "                \"\"\"\n",
    "        db_bands = []\n",
    "        for band in self.bands:\n",
    "            if 'amplitude' in self.unit:\n",
    "                db_bands.append(20*np.log(band))\n",
    "            else:\n",
    "                db_bands.append(10 * np.log(band))\n",
    "\n",
    "        return SarImage(db_bands, mission=self.mission, time=self.time,\n",
    "                        footprint=self.footprint, product_meta=self.product_meta,\n",
    "                        band_names=self.band_names, calibration_tables=self.calibration_tables,\n",
    "                        geo_tie_point=self.geo_tie_point, band_meta=self.band_meta,\n",
    "                        unit=(self.unit+' dB'))\n",
    "\n",
    "    def boxcar(self, kernel_size, **kwargs):\n",
    "        \"\"\"Simple (kernel_size x kernel_size) boxcar filter.\n",
    "            Args:\n",
    "                kernel_size(int): size of kernel\n",
    "                **kwargs: Additional arguments passed to scipy.ndimage.convolve\n",
    "            Returns:\n",
    "                Filtered image\n",
    "        \"\"\"\n",
    "\n",
    "        filter_bands = []\n",
    "        for band in self.bands:\n",
    "            filter_bands.append(tools.boxcar(band, kernel_size, **kwargs))\n",
    "\n",
    "        return SarImage(filter_bands, mission=self.mission, time=self.time,\n",
    "                        footprint=self.footprint, product_meta=self.product_meta,\n",
    "                        band_names=self.band_names, calibration_tables=self.calibration_tables,\n",
    "                        geo_tie_point=self.geo_tie_point, band_meta=self.band_meta,\n",
    "                        unit=self.unit)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save the SarImage object in a folder at path.\n",
    "            Args:\n",
    "                path(str): Path of the folder where the the SarImage is saved.\n",
    "                        Note that the folder is created and must not exist in advance\n",
    "            Raises:\n",
    "                ValueError: There already exist a folder at path\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if folder exists\n",
    "        if os.path.exists(path):\n",
    "            print('please give a path that is not used')\n",
    "            raise ValueError\n",
    "\n",
    "        # make folder\n",
    "        os.makedirs(path)\n",
    "\n",
    "        # save elements in separate files\n",
    "\n",
    "        # product_meta\n",
    "        file_path = os.path.join(path, 'product_meta.pkl')\n",
    "        pickle.dump(self.product_meta, open(file_path, \"wb\"))\n",
    "\n",
    "        # unit\n",
    "        file_path = os.path.join(path, 'unit.pkl')\n",
    "        pickle.dump(self.unit, open(file_path, \"wb\"))\n",
    "\n",
    "        # footprint\n",
    "        file_path = os.path.join(path, 'footprint.pkl')\n",
    "        pickle.dump(self.footprint, open(file_path, \"wb\"))\n",
    "\n",
    "        # geo_tie_point\n",
    "        file_path = os.path.join(path, 'geo_tie_point.pkl')\n",
    "        pickle.dump(self.geo_tie_point, open(file_path, \"wb\"))\n",
    "\n",
    "        # band_names\n",
    "        file_path = os.path.join(path, 'band_names.pkl')\n",
    "        pickle.dump(self.band_names, open(file_path, \"wb\"))\n",
    "\n",
    "        # band_meta\n",
    "        file_path = os.path.join(path, 'band_meta.pkl')\n",
    "        pickle.dump(self.band_meta, open(file_path, \"wb\"))\n",
    "\n",
    "        # bands\n",
    "        file_path = os.path.join(path, 'bands.pkl')\n",
    "        pickle.dump(self.bands, open(file_path, \"wb\"))\n",
    "\n",
    "        # reduce size of calibration_tables list\n",
    "        reduced_calibration = []\n",
    "        for i in range(len(self.bands)):\n",
    "            cal = self.calibration_tables[i]\n",
    "\n",
    "            # Get mask of rows in the image.\n",
    "            index_row = (0 < cal['row']) & (cal['row'] < self.bands[i].shape[0])\n",
    "            # Include one extra row on each side of the image to ensure interpolation\n",
    "            index_row[1:] = index_row[1:] + index_row[:-1]\n",
    "            index_row[:-1] = index_row[:-1] + index_row[1:]\n",
    "\n",
    "            # Get mask of column in the image\n",
    "            index_column = (0 < cal['column']) & (cal['column'] < self.bands[i].shape[1])\n",
    "            # Include one extra column on each side of the image to ensure interpolation\n",
    "            index_column[1:] = index_column[1:] + index_column[:-1]\n",
    "            index_column[:-1] = index_column[:-1] + index_column[1:]\n",
    "\n",
    "            # Get the relevant calibration_tables values\n",
    "            reduced_cal_i = {\n",
    "                \"abs_calibration_const\": cal[\"abs_calibration_const\"],\n",
    "                \"row\": cal[\"row\"][index_row],\n",
    "                \"column\": cal[\"column\"][index_column],\n",
    "                \"azimuth_time\": cal[\"azimuth_time\"][index_row, :][:, index_column],\n",
    "                \"sigma_0\": cal[\"sigma_0\"][index_row, :][:, index_column],\n",
    "                \"beta_0\": cal[\"beta_0\"][index_row, :][:, index_column],\n",
    "                \"gamma\": cal[\"gamma\"][index_row, :][:, index_column],\n",
    "                \"dn\": cal[\"dn\"][index_row, :][:, index_column]\n",
    "            }\n",
    "\n",
    "            reduced_calibration.append(reduced_cal_i)\n",
    "\n",
    "        # calibration_tables\n",
    "        file_path = os.path.join(path, 'calibration_tables.pkl')\n",
    "        pickle.dump(reduced_calibration, open(file_path, \"wb\"))\n",
    "\n",
    "        return\n",
    "\n",
    "    def pop(self, index=-1):\n",
    "        \"\"\"\n",
    "        Remove and return band at index (default last).\n",
    "        Raises IndexError if list is empty or index is out of range.\n",
    "        \"\"\"\n",
    "\n",
    "        band = self.bands.pop(index)\n",
    "        name = self.band_names.pop(index)\n",
    "        calibration_tables = self.calibration_tables.pop(index)\n",
    "        geo_tie_point = self.geo_tie_point.pop(index)\n",
    "        band_meta = self.band_meta.pop()\n",
    "\n",
    "        return SarImage([band], mission=self.mission, time=self.time,\n",
    "                        footprint=self.footprint, product_meta=self.product_meta,\n",
    "                        band_names=[name], calibration_tables=[calibration_tables],\n",
    "                        geo_tie_point=[geo_tie_point], band_meta=[band_meta],\n",
    "                        unit=self.unit)\n",
    "\n",
    "    def get_band(self, index):\n",
    "        \"\"\"\n",
    "        Return SarImage of band at index (default last).\n",
    "        \"\"\"\n",
    "\n",
    "        band = self.bands[index]\n",
    "        name = self.band_names[index]\n",
    "        calibration_tables = self.calibration_tables[index]\n",
    "        geo_tie_point = self.geo_tie_point[index]\n",
    "        band_meta = self.band_meta[index]\n",
    "\n",
    "        return SarImage([band], mission=self.mission, time=self.time,\n",
    "                        footprint=self.footprint, product_meta=self.product_meta,\n",
    "                        band_names=[name], calibration_tables=[calibration_tables],\n",
    "                        geo_tie_point=[geo_tie_point], band_meta=[band_meta],\n",
    "                        unit=self.unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.etree.ElementTree\n",
    "import warnings\n",
    "import datetime\n",
    "import lxml.etree\n",
    "\n",
    "def _load_calibration(path):\n",
    "    \"\"\"Load sentinel 1 calibration_table file as dictionary from PATH.\n",
    "    The calibration_table file should be as included in .SAFE format\n",
    "    retrieved from: https://scihub.copernicus.eu/\n",
    "    Args:\n",
    "        path: The path to the calibration_table file\n",
    "    Returns:\n",
    "        calibration_table: A dictionary with calibration_table constants\n",
    "            {\"abs_calibration_const\": float(),\n",
    "            \"row\": np.array(int),\n",
    "            \"column\": np.array(int),\n",
    "            \"azimuth_time\": np.array(datetime64[us]),\n",
    "            \"sigma_0\": np.array(float),\n",
    "            \"beta_0\": np.array(float),\n",
    "            \"gamma\": np.array(float),\n",
    "            \"dn\": np.array(float),}\n",
    "        info: A dictionary with the meta data given in 'adsHeader'\n",
    "            {child[0].tag: child[0].text,\n",
    "             child[1].tag: child[1].text,\n",
    "             ...}\n",
    "    \"\"\"\n",
    "    # open xml file\n",
    "    tree = xml.etree.ElementTree.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find info\n",
    "    info_xml = root.findall('adsHeader')\n",
    "    if len(info_xml) == 1:\n",
    "        info = {}\n",
    "        for child in info_xml[0]:\n",
    "            info[child.tag] = child.text\n",
    "    else:\n",
    "        warnings.warn('Warning adsHeader not found')\n",
    "        info = None\n",
    "\n",
    "    # Find calibration_table list\n",
    "    cal_vectors = root.findall('calibrationVectorList')\n",
    "    if len(cal_vectors) == 1:\n",
    "        cal_vectors = cal_vectors[0]\n",
    "    else:\n",
    "        warnings.warn('Error loading calibration_table list')\n",
    "        return None, info\n",
    "\n",
    "    # get pixels from first vector\n",
    "    pixel = np.array(list(map(int, cal_vectors[0][2].text.split())))\n",
    "    # initialize arrays\n",
    "    azimuth_time = np.empty([len(cal_vectors),len(pixel)], dtype='datetime64[us]')\n",
    "    line = np.empty([len(cal_vectors)], dtype=int)\n",
    "    sigma_0 = np.empty([len(cal_vectors),len(pixel)], dtype=float)\n",
    "    beta_0 = np.empty([len(cal_vectors),len(pixel)], dtype=float)\n",
    "    gamma = np.empty([len(cal_vectors),len(pixel)], dtype=float)\n",
    "    dn = np.empty([len(cal_vectors),len(pixel)], dtype=float)\n",
    "\n",
    "    # get data\n",
    "    for i, cal_vec in enumerate(cal_vectors):\n",
    "        pixel_i = np.array(list(map(int, cal_vec[2].text.split())))\n",
    "        if not np.array_equal(pixel,pixel_i):\n",
    "            warnings.warn('Warning in _load_calibration. The calibration_table data is not on a proper grid')\n",
    "        azimuth_time[i,:] = np.datetime64(cal_vec[0].text)\n",
    "        line[i] = int(cal_vec[1].text)\n",
    "        sigma_0[i,:] = np.array(list(map(float, cal_vec[3].text.split())))\n",
    "        beta_0[i,:] = np.array(list(map(float, cal_vec[4].text.split())))\n",
    "        gamma[i,:] = np.array(list(map(float, cal_vec[5].text.split())))\n",
    "        dn[i,:] = np.array(list(map(float, cal_vec[6].text.split())))\n",
    "\n",
    "    # Combine calibration_table info\n",
    "    calibration_table = {\n",
    "        \"abs_calibration_const\": float(root[1][0].text),\n",
    "        \"row\": line,\n",
    "        \"column\": pixel,\n",
    "        \"azimuth_time\": azimuth_time,\n",
    "        \"sigma_0\": sigma_0,\n",
    "        \"beta_0\": beta_0,\n",
    "        \"gamma\": gamma,\n",
    "        \"dn\": dn,\n",
    "    }\n",
    "\n",
    "    return calibration_table, info\n",
    "\n",
    "\n",
    "def _load_meta(SAFE_path):\n",
    "    \"\"\"Load manifest.safe as dictionary from SAFE_path.\n",
    "    The manifest.safe file should be as included in .SAFE format\n",
    "    retrieved from: https://scihub.copernicus.eu/\n",
    "    Args:\n",
    "        path: The path to the manifest.safe file\n",
    "    Returns:\n",
    "        metadata: A dictionary with meta_data\n",
    "            example:\n",
    "            {'mode': 'EW',\n",
    "             'swath': ['EW'],\n",
    "             'instrument_config': 1,\n",
    "             'mission_data_ID': '110917',\n",
    "             'polarisation': ['HH', 'HV'],\n",
    "             'product_class': 'S',\n",
    "             'product_composition': 'Slice',\n",
    "             'product_type': 'GRD',\n",
    "             'product_timeliness': 'Fast-24h',\n",
    "             'slice_product_flag': 'true',\n",
    "             'segment_start_time': datetime.datetime(2019, 1, 17, 19, 12, 32, 164986),\n",
    "             'slice_number': 4,\n",
    "             'total_slices': 4,\n",
    "             'footprint': {'latitude': array([69.219566, 69.219566, 69.219566, 69.219566]),\n",
    "                            'longitude': array([-35.149223, -35.149223, -35.149223, -35.149223])},\n",
    "             'nssdc_identifier': '2016-025A',\n",
    "             'mission': 'SENTINEL-1B',\n",
    "             'orbit_number': array([14538, 14538]),\n",
    "             'relative_orbit_number': array([162, 162]),\n",
    "             'cycle_number': 89,\n",
    "             'phase_identifier': 1,\n",
    "             'start_time': datetime.datetime(2019, 1, 17, 19, 15, 36, 268585),\n",
    "             'stop_time': datetime.datetime(2019, 1, 17, 19, 16, 25, 598196),\n",
    "             'pass': 'ASCENDING',\n",
    "             'ascending_node_time': datetime.datetime(2019, 1, 17, 18, 57, 16, 851007),\n",
    "             'start_time_ANX': 1099418.0,\n",
    "             'stop_time_ANX': 1148747.0}\n",
    "            error: List of dictionary keys that was not found.\n",
    "    \"\"\"\n",
    "    # Sorry the code look like shit but I do not like the file format\n",
    "    # and I do not trust that ESA will keep the structure.\n",
    "    # This is the reason for all the if statements and the error list\n",
    "\n",
    "    # Open the xml like file\n",
    "    with open(SAFE_path) as f:\n",
    "        safe_test = f.read()\n",
    "    safe_string = safe_test.encode(errors='ignore')\n",
    "    safe_xml = lxml.etree.fromstring(safe_string)\n",
    "\n",
    "    # Initialize results\n",
    "    metadata = {}\n",
    "    error = []\n",
    "\n",
    "    # Prefixes used in the tag of the file. Do not ask me why the use them\n",
    "    prefix1 = '{http://www.esa.int/safe/sentinel-1.0}'\n",
    "    prefix2 = '{http://www.esa.int/safe/sentinel-1.0/sentinel-1}'\n",
    "    prefix3 = '{http://www.esa.int/safe/sentinel-1.0/sentinel-1/sar/level-1}'\n",
    "\n",
    "    # Put the data into the metadata\n",
    "\n",
    "    # Get nssdc_identifier\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'nssdcIdentifier')]\n",
    "    if len(values) == 1:\n",
    "        metadata['nssdc_identifier'] = values[0].text\n",
    "    else:\n",
    "        error.append('nssdcIdentifier')\n",
    "\n",
    "    # Get mission\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'familyName')]\n",
    "    values2 = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'number')]\n",
    "    if (len(values) > 0) & (len(values2) == 1):\n",
    "        metadata['mission'] = values[0].text + values2[0].text\n",
    "    else:\n",
    "        error.append('mission')\n",
    "\n",
    "    # get orbit_number\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'orbitNumber')]\n",
    "    if len(values) == 2:\n",
    "        metadata['orbit_number'] = np.array([int(values[0].text), int(values[1].text)])\n",
    "    else:\n",
    "        error.append('orbit_number')\n",
    "\n",
    "    # get relative_orbit_number\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'relativeOrbitNumber')]\n",
    "    if len(values) == 2:\n",
    "        metadata['relative_orbit_number'] = np.array([int(values[0].text), int(values[1].text)])\n",
    "    else:\n",
    "        error.append('relative_orbit_number')\n",
    "\n",
    "    # get cycle_number\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'cycleNumber')]\n",
    "    if len(values) == 1:\n",
    "        metadata['cycle_number'] = int(values[0].text)\n",
    "    else:\n",
    "        error.append('cycle_number')\n",
    "\n",
    "    # get phase_identifier\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'phaseIdentifier')]\n",
    "    if len(values) == 1:\n",
    "        metadata['phase_identifier'] = int(values[0].text)\n",
    "    else:\n",
    "        error.append('phase_identifier')\n",
    "\n",
    "    # get start_time\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'startTime')]\n",
    "    if len(values) == 1:\n",
    "        t = values[0].text\n",
    "        metadata['start_time'] = datetime.datetime(int(t[:4]), int(t[5:7]), int(t[8:10]), int(t[11:13]),\n",
    "                                                   int(t[14:16]), int(t[17:19]), int(float(t[19:]) * 10 ** 6))\n",
    "    else:\n",
    "        error.append('start_time')\n",
    "\n",
    "    # get stop_time\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix1 + 'stopTime')]\n",
    "    if len(values) == 1:\n",
    "        t = values[0].text\n",
    "        metadata['stop_time'] = datetime.datetime(int(t[:4]), int(t[5:7]), int(t[8:10]), int(t[11:13]), int(t[14:16]),\n",
    "                                                  int(t[17:19]), int(float(t[19:]) * 10 ** 6))\n",
    "    else:\n",
    "        error.append('stop_time')\n",
    "\n",
    "    # get pass\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix2 + 'pass')]\n",
    "    if len(values) == 1:\n",
    "        metadata['pass'] = values[0].text\n",
    "    else:\n",
    "        error.append('pass')\n",
    "\n",
    "    # get ascending_node_time\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix2 + 'ascendingNodeTime')]\n",
    "    if len(values) == 1:\n",
    "        t = values[0].text\n",
    "        metadata['ascending_node_time'] = datetime.datetime(int(t[:4]), int(t[5:7]), int(t[8:10]), int(t[11:13]),\n",
    "                                                            int(t[14:16]), int(t[17:19]), int(float(t[19:]) * 10 ** 6))\n",
    "    else:\n",
    "        error.append('ascending_node_time')\n",
    "\n",
    "    # get start_time_ANX\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix2 + 'startTimeANX')]\n",
    "    if len(values) == 1:\n",
    "        metadata['start_time_ANX'] = float(values[0].text)\n",
    "    else:\n",
    "        error.append('start_time_ANX')\n",
    "\n",
    "    # get stop_time_ANX\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix2 + 'stopTimeANX')]\n",
    "    if len(values) == 1:\n",
    "        metadata['stop_time_ANX'] = float(values[0].text)\n",
    "    else:\n",
    "        error.append('stop_time_ANX')\n",
    "\n",
    "    # get mode\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'mode')]\n",
    "    if len(values) == 1:\n",
    "        metadata['mode'] = values[0].text\n",
    "    else:\n",
    "        error.append('mode')\n",
    "\n",
    "    # get swath\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'swath')]\n",
    "    if len(values) > 0:\n",
    "        metadata['swath'] = [child.text for child in values]\n",
    "    else:\n",
    "        error.append('swath')\n",
    "\n",
    "    # get instrument_config\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'instrumentConfigurationID')]\n",
    "    if len(values) == 1:\n",
    "        metadata['instrument_config'] = int(values[0].text)\n",
    "    else:\n",
    "        error.append('instrument_config')\n",
    "\n",
    "    # get mission_data_ID\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'missionDataTakeID')]\n",
    "    if len(values) == 1:\n",
    "        metadata['mission_data_ID'] = values[0].text\n",
    "    else:\n",
    "        error.append('mission_data_ID')\n",
    "\n",
    "    # get polarisation\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'transmitterReceiverPolarisation')]\n",
    "    if len(values) > 0:\n",
    "        metadata['polarisation'] = [child.text for child in values]\n",
    "    else:\n",
    "        error.append('polarisation')\n",
    "\n",
    "    # get product_class\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'productClass')]\n",
    "    if len(values) == 1:\n",
    "        metadata['product_class'] = values[0].text\n",
    "    else:\n",
    "        error.append('product_class')\n",
    "\n",
    "    # get product_composition\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'productComposition')]\n",
    "    if len(values) == 1:\n",
    "        metadata['product_composition'] = values[0].text\n",
    "    else:\n",
    "        error.append('product_composition')\n",
    "\n",
    "    # get product_type\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'productType')]\n",
    "    if len(values) == 1:\n",
    "        metadata['product_type'] = values[0].text\n",
    "    else:\n",
    "        error.append('product_type')\n",
    "\n",
    "    # get product_timeliness\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'productTimelinessCategory')]\n",
    "    if len(values) == 1:\n",
    "        metadata['product_timeliness'] = values[0].text\n",
    "    else:\n",
    "        error.append('product_timeliness')\n",
    "\n",
    "    # get slice_product_flag\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'sliceProductFlag')]\n",
    "    if len(values) == 1:\n",
    "        metadata['slice_product_flag'] = values[0].text\n",
    "    else:\n",
    "        error.append('slice_product_flag')\n",
    "\n",
    "    # get segment_start_time\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'segmentStartTime')]\n",
    "    if len(values) == 1:\n",
    "        t = values[0].text\n",
    "        metadata['segment_start_time'] = datetime.datetime(int(t[:4]), int(t[5:7]), int(t[8:10]), int(t[11:13]),\n",
    "                                                           int(t[14:16]), int(t[17:19]), int(float(t[19:]) * 10 ** 6))\n",
    "    else:\n",
    "        error.append('segment_start_time')\n",
    "\n",
    "    # get slice_number\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'sliceNumber')]\n",
    "    if len(values) == 1:\n",
    "        metadata['slice_number'] = int(values[0].text)\n",
    "    else:\n",
    "        error.append('slice_number')\n",
    "\n",
    "    # get total_slices\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + prefix3 + 'totalSlices')]\n",
    "    if len(values) == 1:\n",
    "        metadata['total_slices'] = int(values[0].text)\n",
    "    else:\n",
    "        error.append('total_slices')\n",
    "\n",
    "    # get footprint\n",
    "    values = [elem for elem in safe_xml.iterfind(\".//\" + '{http://www.opengis.net/gml}coordinates')]\n",
    "    if len(values) == 1:\n",
    "        coordinates = values[0].text.split()\n",
    "        lat = np.zeros(4)\n",
    "        lon = np.zeros(4)\n",
    "        for i in range(0, len(coordinates)):\n",
    "            coord_i = coordinates[i].split(',')\n",
    "            lat[i] = float(coord_i[0])\n",
    "            lon[i] = float(coord_i[1])\n",
    "        footprint = {'latitude': lat, 'longitude': lon}\n",
    "        metadata['footprint'] = footprint\n",
    "    else:\n",
    "        error.append('footprint')\n",
    "\n",
    "    return metadata, error\n",
    "\n",
    "\n",
    "def _load_annotation(path):\n",
    "    \"\"\"Load sentinel 1 annotation file as dictionary from PATH.\n",
    "     The annotation file should be as included in .SAFE format\n",
    "     retrieved from: https://scihub.copernicus.eu/\n",
    "     Note that the file contains more information. Only the relevant have been chosen\n",
    "     Args:\n",
    "         path: The path to the annotation file\n",
    "     Returns:\n",
    "         geo_locations: A dictionary with geo location tie-points\n",
    "             {'azimuth_time': np.array(datetime64[us]),\n",
    "            'slant_range_time': np.array(float),\n",
    "            'row': np.array(int),\n",
    "            'column': np.array(int),\n",
    "            'latitude': np.array(float),\n",
    "            'longitude': np.array(float),\n",
    "            'height': np.array(float),\n",
    "            'incidence_angle': np.array(float),\n",
    "            'elevation_angle': np.array(float)}\n",
    "         info: A dictionary with the meta data given in 'adsHeader'\n",
    "             {child[0].tag: child[0].text,\n",
    "              child[1].tag: child[1].text,\n",
    "              ...}\n",
    "     \"\"\"\n",
    "\n",
    "    # open xml file\n",
    "    tree = xml.etree.ElementTree.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find info\n",
    "    info_xml = root.findall('adsHeader')\n",
    "    if len(info_xml) == 1:\n",
    "        info = {}\n",
    "        for child in info_xml[0]:\n",
    "            info[child.tag] = child.text\n",
    "    else:\n",
    "        warnings.warn('Warning adsHeader not found')\n",
    "        info = None\n",
    "\n",
    "    # Find geo location list\n",
    "    geo_points = root.findall('geolocationGrid')\n",
    "    if len(geo_points) == 1:\n",
    "        geo_points = geo_points[0][0]\n",
    "    else:\n",
    "        warnings.warn('Warning geolocationGrid not found')\n",
    "        return None, None\n",
    "\n",
    "    # initialize arrays\n",
    "    n_points = len(geo_points)\n",
    "    azimuth_time = np.empty(n_points, dtype='datetime64[us]')\n",
    "    slant_range_time = np.zeros(n_points, dtype=float)\n",
    "    line = np.zeros(n_points, dtype=int)\n",
    "    pixel = np.zeros(n_points, dtype=int)\n",
    "    latitude = np.zeros(n_points, dtype=float)\n",
    "    longitude = np.zeros(n_points, dtype=float)\n",
    "    height = np.zeros(n_points, dtype=float)\n",
    "    incidence_angle = np.zeros(n_points, dtype=float)\n",
    "    elevation_angle = np.zeros(n_points, dtype=float)\n",
    "\n",
    "    # get the data\n",
    "    for i in range(0, n_points):\n",
    "        point = geo_points[i]\n",
    "\n",
    "        azimuth_time[i] = np.datetime64(point[0].text)\n",
    "        slant_range_time[i] = float(point[1].text)\n",
    "        line[i] = int(point[2].text)\n",
    "        pixel[i] = int(point[3].text)\n",
    "        latitude[i] = float(point[4].text)\n",
    "        longitude[i] = float(point[5].text)\n",
    "        height[i] = float(point[6].text)\n",
    "        incidence_angle[i] = float(point[7].text)\n",
    "        elevation_angle[i] = float(point[8].text)\n",
    "\n",
    "    # Combine geo_locations info\n",
    "    geo_locations = {\n",
    "        'azimuth_time': azimuth_time,\n",
    "        'slant_range_time': slant_range_time,\n",
    "        'row': line,\n",
    "        'column': pixel,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'height': height,\n",
    "        'incidence_angle': incidence_angle,\n",
    "        'elevation_angle': elevation_angle\n",
    "    }\n",
    "\n",
    "    return geo_locations, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rasterio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e975e3752b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msarpy_class\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSarImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rasterio'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from itertools import compress\n",
    "import rasterio\n",
    "\n",
    "from .sarpy_class import SarImage\n",
    "from . import s1\n",
    "from .get_functions import get_index_v2, get_coordinate\n",
    "\n",
    "\n",
    "def s1_load(path, polarisation='all', location=None, size=None):\n",
    "    \"\"\"Function to load SAR image into SarImage python object.\n",
    "        Currently supports: unzipped Sentinel 1 GRDH products\n",
    "        Args:\n",
    "            path(number): Path to the folder containing the SAR image\n",
    "                    as retrieved from: https://scihub.copernicus.eu/\n",
    "            polarisation(list of str): List of polarisations to load.\n",
    "            location(array/list): [latitude,longitude] Location to center the image.\n",
    "                                    If None the entire Image is loaded\n",
    "            size(array/list): [width, height] Extend of image to load.\n",
    "                                    If None the entire Image is loaded\n",
    "        Returns:\n",
    "            SarImage: object with the SAR measurements and meta data from path. Meta data index\n",
    "                    and foot print are adjusted to the window\n",
    "        Raises:\n",
    "            ValueError: Location not in image\n",
    "        \"\"\"\n",
    "    # manifest.safe\n",
    "    path_safe = os.path.join(path, 'manifest.safe')\n",
    "    meta, error = s1._load_meta(path_safe)\n",
    "\n",
    "    # annotation\n",
    "    ls_annotation = os.listdir(os.path.join(path, 'annotation'))\n",
    "    xml_files = [file[-3:] == 'xml' for file in ls_annotation]\n",
    "    xml_files = list(compress(ls_annotation, xml_files))\n",
    "    annotation_temp = [s1._load_annotation(os.path.join(path, 'annotation', file)) for file in xml_files]\n",
    "\n",
    "    # calibration_tables\n",
    "    path_cal = os.path.join(path, 'annotation', 'calibration')\n",
    "    ls_cal = os.listdir(path_cal)\n",
    "    cal_files = [file[:11] == 'calibration' for file in ls_cal]\n",
    "    cal_files = list(compress(ls_cal, cal_files))\n",
    "    calibration_temp = [s1._load_calibration(os.path.join(path_cal, file)) for file in cal_files]\n",
    "\n",
    "    # measurement\n",
    "    measurement_path = os.path.join(path, 'measurement')\n",
    "    ls_meas = os.listdir(measurement_path)\n",
    "    tiff_files = [file[-4:] == 'tiff' for file in ls_meas]\n",
    "    tiff_files = list(compress(ls_meas, tiff_files))\n",
    "    with warnings.catch_warnings(): # Ignore the \"NotGeoreferencedWarning\" when opening the tiff\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        measurement_temp = [rasterio.open(os.path.join(measurement_path, file)) for file in tiff_files]\n",
    "\n",
    "    # Check if polarisation is given\n",
    "    if polarisation == 'all':\n",
    "        polarisation = meta['polarisation']\n",
    "    else:\n",
    "        polarisation = [elem.upper() for elem in polarisation]\n",
    "\n",
    "    # only take bands of interest and sort\n",
    "    n_bands = len(polarisation)\n",
    "    calibration_tables = [None] * n_bands\n",
    "    geo_tie_point = [None] * n_bands\n",
    "    band_meta = [None] * n_bands\n",
    "    measurement = [None] * n_bands\n",
    "\n",
    "    for i in range(n_bands):\n",
    "\n",
    "        for idx, file in enumerate(tiff_files):\n",
    "            if file.split('-')[3].upper() == polarisation[i]:\n",
    "                measurement[i] = measurement_temp[idx]\n",
    "\n",
    "        for band in calibration_temp:\n",
    "            if band[1]['polarisation'] == polarisation[i]:\n",
    "                calibration_tables[i] = band[0]\n",
    "\n",
    "        for band in annotation_temp:\n",
    "            if band[1]['polarisation'] == polarisation[i]:\n",
    "                geo_tie_point[i] = band[0]\n",
    "                band_meta[i] = band[1]\n",
    "\n",
    "    # Check that there is one band in each tiff\n",
    "    for i in range(n_bands):\n",
    "        if measurement[i].count != 1:\n",
    "            warnings.warn('Warning tiff file contains several bands. First band read from each tiff file')\n",
    "\n",
    "    if (location is None) or (size is None):\n",
    "        bands = [image.read(1) for image in measurement]\n",
    "    else:\n",
    "        # Check location is in foot print\n",
    "        maxlat = meta['footprint']['latitude'].max()\n",
    "        minlat = meta['footprint']['latitude'].min()\n",
    "        maxlong = meta['footprint']['longitude'].max()\n",
    "        minlong = meta['footprint']['longitude'].min()\n",
    "\n",
    "        if not (minlat < location[0] < maxlat) & (minlong < location[1] < maxlong):\n",
    "            raise ValueError('Location not inside the footprint')\n",
    "\n",
    "        # get the index\n",
    "        row = np.zeros(len(geo_tie_point), dtype=int)\n",
    "        column = np.zeros(len(geo_tie_point), dtype=int)\n",
    "        for i in range(len(geo_tie_point)):\n",
    "            lat_grid = geo_tie_point[i]['latitude']\n",
    "            long_grid = geo_tie_point[i]['longitude']\n",
    "            row_grid = geo_tie_point[i]['row']\n",
    "            column_grid = geo_tie_point[i]['column']\n",
    "            row[i], column[i] = get_index_v2(location[0], location[1], lat_grid, long_grid, row_grid, column_grid)\n",
    "        # check if index are the same for all bands\n",
    "        if (abs(row.max() - row.min()) > 0.5) or (abs(column.max() - column.min()) > 0.5):\n",
    "            warnings.warn('Warning different index found for each band. First index returned')\n",
    "\n",
    "        # Find the window\n",
    "        row_index_min = row[0] - int(size[0]/2)\n",
    "        row_index_max = row[0] + int(size[0]/2)\n",
    "\n",
    "        column_index_min = column[0] - int(size[1]/2)\n",
    "        column_index_max = column[0] + int(size[1]/2)\n",
    "\n",
    "        # Check if window is in image\n",
    "        if row_index_max < 0 or column_index_max < 0:\n",
    "            raise ValueError('Error window not in image ')\n",
    "\n",
    "        if row_index_min < 0:\n",
    "            warnings.warn('Extend out of image. Window constrained ')\n",
    "            row_index_min = 0\n",
    "\n",
    "        if column_index_min < 0:\n",
    "            warnings.warn('Extend out of image. Window constrained ')\n",
    "            column_index_min = 0\n",
    "\n",
    "        for image in measurement:\n",
    "            if row_index_min > image.height or column_index_min > image.width:\n",
    "                raise ValueError('Error window not in image')\n",
    "\n",
    "            if row_index_max > image.height:\n",
    "                warnings.warn('Extend out of image. Window constrained ')\n",
    "                row_index_max = image.height\n",
    "\n",
    "            if column_index_max > image.width:\n",
    "                warnings.warn('Extend out of image. Window constrained ')\n",
    "                column_index_max = image.width\n",
    "\n",
    "        # Adjust footprint to window\n",
    "        footprint_lat = np.zeros(4)\n",
    "        footprint_long = np.zeros(4)\n",
    "        window = ((row_index_min, row_index_max), (column_index_min, column_index_max))\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                lat_i, long_i = get_coordinate(window[0][i], window[1][j], geo_tie_point[0]['latitude'],\n",
    "                                               geo_tie_point[0]['longitude'], geo_tie_point[0]['row'],\n",
    "                                               geo_tie_point[0]['column'])\n",
    "                footprint_lat[2 * i + j] = lat_i\n",
    "                footprint_long[2 * i + j] = long_i\n",
    "\n",
    "        meta['footprint']['latitude'] = footprint_lat\n",
    "        meta['footprint']['longitude'] = footprint_long\n",
    "\n",
    "        # Adjust geo_tie_point, calibration_tables\n",
    "        for i in range(n_bands):\n",
    "            geo_tie_point[i]['row'] = geo_tie_point[i]['row'] - row_index_min\n",
    "            geo_tie_point[i]['column'] = geo_tie_point[i]['column'] - column_index_min\n",
    "\n",
    "            calibration_tables[i]['row'] = calibration_tables[i]['row'] - row_index_min\n",
    "            calibration_tables[i]['column'] = calibration_tables[i]['column'] - column_index_min\n",
    "\n",
    "        # load the data window\n",
    "        bands = [image.read(1, window=window) for image in measurement]\n",
    "\n",
    "    return SarImage(bands, mission=meta['mission'], time=meta['start_time'],\n",
    "                    footprint=meta['footprint'], product_meta=meta,\n",
    "                    band_names=polarisation, calibration_tables=calibration_tables,\n",
    "                    geo_tie_point=geo_tie_point, band_meta=band_meta, unit='raw amplitude')\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    \"\"\" Load SarImage saved with the SarImage save method (img.save(path)).\n",
    "        Args:\n",
    "            path(str): Path to the folder where SarImage is saved.\n",
    "        Returns:\n",
    "            SarImage\n",
    "        \"\"\"\n",
    "\n",
    "    # product_meta\n",
    "    file_path = os.path.join(path,'product_meta.pkl')\n",
    "    product_meta = pickle.load( open( file_path, \"rb\" ) )\n",
    "\n",
    "    # unit\n",
    "    file_path = os.path.join(path,'unit.pkl')\n",
    "    unit = pickle.load( open( file_path, \"rb\" ) )\n",
    "\n",
    "    # footprint\n",
    "    file_path = os.path.join(path,'footprint.pkl')\n",
    "    footprint = pickle.load(open( file_path, \"rb\" ) )\n",
    "\n",
    "    # geo_tie_point\n",
    "    file_path = os.path.join(path,'geo_tie_point.pkl')\n",
    "    geo_tie_point= pickle.load( open( file_path, \"rb\" ) )\n",
    "\n",
    "    # band_names\n",
    "    file_path = os.path.join(path,'band_names.pkl')\n",
    "    band_names = pickle.load(open( file_path, \"rb\" ) )\n",
    "\n",
    "    # band_meta\n",
    "    file_path = os.path.join(path,'band_meta.pkl')\n",
    "    band_meta = pickle.load(open( file_path, \"rb\" ) )\n",
    "\n",
    "    # bands\n",
    "    file_path = os.path.join(path,'bands.pkl')\n",
    "    bands = pickle.load(open( file_path, \"rb\" ) )\n",
    "\n",
    "    # calibration_tables\n",
    "    file_path = os.path.join(path,'calibration_tables.pkl')\n",
    "    calibration_tables = pickle.load(open( file_path, \"rb\" ) )\n",
    "    \n",
    "    return SarImage(bands, mission=product_meta['mission'], time=product_meta['start_time'],\n",
    "                    footprint=footprint, product_meta=product_meta,\n",
    "                    band_names=band_names, calibration_tables=calibration_tables,\n",
    "                    geo_tie_point=geo_tie_point, band_meta=band_meta, unit=unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def calibration(band, rows, columns, calibration_values, tiles=4):\n",
    "    \"\"\"Calibrates image using linear interpolation.\n",
    "    See https://sentinel.esa.int/documents/247904/685163/S1-Radiometric-Calibration-V1.0.pdf\n",
    "    Args:\n",
    "        band(2d numpy array): The non calibrated image\n",
    "        rows(number): rows of calibration point\n",
    "        columns(number): columns of calibration point\n",
    "        calibration_values(2d numpy array): grid of calibration values\n",
    "        tiles(int): number of tiles the image is divided into. This saves memory but reduce speed a bit\n",
    "    Returns:\n",
    "        calibrated image (2d numpy array)\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create interpolation function\n",
    "    f = RegularGridInterpolator((rows, columns), calibration_values)\n",
    "\n",
    "    result = np.zeros(band.shape)\n",
    "    # Calibrate one tile at the time\n",
    "    column_start = 0\n",
    "    column_max = band.shape[1]\n",
    "    for i in range(tiles):\n",
    "        column_end = int(column_max / tiles * (i + 1))\n",
    "        # Create array of point where calibration is needed\n",
    "        column_mesh, row_mesh = np.meshgrid(np.array(range(column_start, column_end)), np.array(range(band.shape[0])))\n",
    "        points = np.array([row_mesh.reshape(-1), column_mesh.reshape(-1)]).T\n",
    "        # Get the image tile and the calibration values for it\n",
    "        img_tile = band[:, column_start:column_end]\n",
    "        img_cal = f(points).reshape(img_tile.shape)\n",
    "        # Set in result\n",
    "        result[:, column_start:column_end] = (img_tile / img_cal)\n",
    "\n",
    "        column_start = column_end\n",
    "    return result ** 2\n",
    "\n",
    "\n",
    "def boxcar(img, kernel_size, **kwargs):\n",
    "    \"\"\"Simple (kernel_size x kernel_size) boxcar filter.\n",
    "    Args:\n",
    "        img(2d numpy array): image\n",
    "        kernel_size(int): size of kernel\n",
    "        **kwargs: Additional arguments passed to scipy.ndimage.convolve\n",
    "    Returns:\n",
    "        Filtered image\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "    # For small kernels simple convolution\n",
    "    if kernel_size < 8:\n",
    "        kernel = np.ones([kernel_size,kernel_size])\n",
    "        box_img = ndimage.convolve(img, kernel, **kwargs)/kernel_size**2\n",
    "\n",
    "    # For large kernels use Separable Filters. (https://www.youtube.com/watch?v=SiJpkucGa1o)\n",
    "    else:\n",
    "        kernel1 = np.ones([kernel_size, 1])\n",
    "        kernel2 = np.ones([1, kernel_size])\n",
    "        box_img = ndimage.convolve(img, kernel1, **kwargs) / kernel_size\n",
    "        box_img = ndimage.convolve(box_img, kernel2, **kwargs) / kernel_size\n",
    "\n",
    "    return box_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def get_coordinate(row, column, lat_gridpoints, long_gridpoints, row_gridpoints, column_gridpoints):\n",
    "    \"\"\"Get coordinate from index by interpolating grid-points\n",
    "    Args:\n",
    "        row(number): index of the row of interest position\n",
    "        column(number): index of the column of interest position\n",
    "        lat_gridpoints(numpy array of length n): Latitude of grid-points\n",
    "        long_gridpoints(numpy array of length n): Longitude of grid-points\n",
    "        row_gridpoints(numpy array of length n): row of grid-points\n",
    "        column_gridpoints(numpy array of length n): column of grid-points\n",
    "    Returns:\n",
    "        lat(float): Latitude of the position\n",
    "        long(float): longitude of the position\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create interpolate functions\n",
    "    points = np.vstack([row_gridpoints, column_gridpoints]).transpose()\n",
    "    lat = float(interpolate.griddata(points, lat_gridpoints, (row, column)))\n",
    "    long = float(interpolate.griddata(points, long_gridpoints, (row, column)))\n",
    "    return lat, long\n",
    "\n",
    "\n",
    "def get_index_v1(lat, long, lat_gridpoints, long_gridpoints, row_gridpoints, column_gridpoints):\n",
    "    \"\"\"Get index of a location by interpolating grid-points\n",
    "    Args:\n",
    "        lat(number): Latitude of the location\n",
    "        long(number): Longitude of location\n",
    "        lat_gridpoints(numpy array of length n): Latitude of grid-points\n",
    "        long_gridpoints(numpy array of length n): Longitude of grid-points\n",
    "        row_gridpoints(numpy array of length n): row of grid-points\n",
    "        column_gridpoints(numpy array of length n): column of grid-points\n",
    "    Returns:\n",
    "        row(int): The row index of the location\n",
    "        column(int): The column index of the location\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "    points = np.vstack([lat_gridpoints, long_gridpoints]).transpose()\n",
    "    row = int(np.round(interpolate.griddata(points, row_gridpoints, (lat, long))))\n",
    "    column = int(np.round(interpolate.griddata(points, column_gridpoints, (lat, long))))\n",
    "    return row, column\n",
    "\n",
    "\n",
    "def get_index_v2(lat, long, lat_gridpoints, long_gridpoints, row_gridpoints, column_gridpoints):\n",
    "    \"\"\"\n",
    "    Same as \"get_index_v1\" but consistent with \"get_coordinate\". Drawback is that it is slower\n",
    "    \"\"\"\n",
    "\n",
    "    # Get an initial guess\n",
    "    row_i, column_i = get_index_v1(lat, long, lat_gridpoints, long_gridpoints, row_gridpoints, column_gridpoints)\n",
    "\n",
    "    # Define a loss function\n",
    "    def loss_function(index):\n",
    "        lat_res, long_res = get_coordinate(index[0], index[1], lat_gridpoints, long_gridpoints, row_gridpoints,\n",
    "                                           column_gridpoints)\n",
    "        return ((lat - lat_res) * 100) ** 2 + ((long - long_res) * 100) ** 2\n",
    "\n",
    "    # Find the index where \"get_coordinate\" gives the closest coordinates\n",
    "    res = minimize(loss_function, [row_i, column_i])\n",
    "\n",
    "    return int(round(res.x[0])), int(round(res.x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
